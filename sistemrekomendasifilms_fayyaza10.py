# -*- coding: utf-8 -*-
"""SistemRekomendasiFilms_Fayyaza10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dUzArcPkut0hoPUzfAu-4ArsCjaPm0xE

# Proyek Sistem Rekomendasi: Rekomendasi Film
- **Nama:** Fayyaza Faryal Anakidanda
- **Email:** anakidandaa@gmail.com
- **ID Dicoding:** anakidandaa

# Overview Proyek

Di era digital, jumlah konten hiburan yang tersedia secara daring meningkat secara signifikan. Platform streaming seperti Netflix, Disney+, dan Amazon Prime menawarkan ribuan judul film dan serial yang dapat diakses kapan saja. Namun, melimpahnya pilihan ini justru menimbulkan tantangan bagi pengguna dalam menemukan tontonan yang sesuai dengan preferensi mereka.

Penelitian menunjukkan bahwa sistem rekomendasi memainkan peran penting dalam membantu pengguna menyaring informasi dan meningkatkan kepuasan dalam mengakses layanan digital (Zhang et al., 2017). Dengan memberikan rekomendasi yang relevan dan dipersonalisasi, sistem ini dapat mempercepat pengambilan keputusan, meningkatkan retensi pengguna, serta memperpanjang durasi keterlibatan mereka di dalam platform.

Penerapan sistem rekomendasi film tidak hanya meningkatkan kenyamanan dalam menelusuri konten, tetapi juga memberikan dampak positif terhadap keterlibatan pengguna dan konsumsi konten. Selain itu, sistem ini juga dapat memperkenalkan penonton pada genre atau film yang sebelumnya tidak mereka ketahui, namun sesuai dengan selera mereka, sehingga mendorong pengalaman menonton yang lebih kaya dan personal.

**Proyek ini penting untuk diselesaikan karena:**
* Membantu pengguna menemukan film yang sesuai minat dengan lebih efisien

* Menunjukkan penerapan sistem rekomendasi dalam dunia nyata, khususnya industri hiburan digital

* Meningkatkan akurasi rekomendasi agar pengalaman pengguna terasa lebih personal dan relevan

Sistem rekomendasi akan dibangun menggunakan dua pendekatan utama: Content-Based Filtering, yang merekomendasikan film berdasarkan kemiripan konten seperti genre, sinopsis, dan kata kunci; serta Collaborative Filtering, yang memanfaatkan data rating pengguna untuk menemukan pola preferensi serupa. Proyek ini menggunakan dataset publik berjudul "The Movies Dataset" yang tersedia di Kaggle.

Referensi:
Zhang, S., Yao, L., Sun, A., & Tay, Y. (2017). Deep Learning Based Recommender System: A Survey and New Perspectives. https://www.researchgate.net/publication/318671349

Herlocker, J. L., Konstan, J. A., Terveen, L. G., & Riedl, J. T. (2004). Evaluating Recommendation Systems. ResearchGate https://www.researchgate.net/publication/226264572

# Business Understanding

## Problem Statements
1. Banyaknya pilihan film yang tersedia secara daring membuat pengguna, terutama yang baru bergabung, kesulitan menemukan tontonan yang sesuai dengan selera mereka. Hal ini dapat mengurangi kenyamanan menonton dan menyebabkan pengguna cepat meninggalkan platform.

2. Sebagian besar platform masih menggunakan pendekatan rekomendasi yang sederhana, seperti menampilkan daftar film populer, tanpa mempertimbangkan preferensi individual. Akibatnya, rekomendasi terasa generik dan kurang relevan secara personal.

3. Pengguna sering memberikan rating atau ulasan setelah menonton sebagai bentuk umpan balik. Namun, data ini belum dimanfaatkan secara optimal untuk membentuk rekomendasi yang lebih akurat dan personal.

##Goals
1. Mengembangkan sistem rekomendasi film yang dapat membantu pengguna baru menemukan film yang relevan, meskipun belum memiliki riwayat tontonan (mengatasi masalah cold-start).

2. Memberikan rekomendasi yang personal dan relevan, sehingga meningkatkan kenyamanan, kepuasan, serta keterlibatan pengguna di dalam platform.

3. Menggabungkan data metadata film dan data interaksi pengguna untuk menghasilkan sistem rekomendasi yang lebih cerdas dan adaptif terhadap pola preferensi.

## Solution Statement
Untuk mencapai tujuan tersebut, proyek ini menerapkan dua pendekatan sistem rekomendasi:

1. Content-Based Filtering
  * Menganalisis karakteristik konten film seperti genre, sinopsis, dan kata kunci. Sistem akan merekomendasikan film yang memiliki kemiripan dengan film yang disukai pengguna. Pendekatan ini sangat cocok untuk pengguna baru.

2. Collaborative Filtering
  * Menggunakan data rating dari pengguna untuk menemukan pola kesamaan antar pengguna. Sistem kemudian merekomendasikan film yang disukai oleh pengguna lain dengan preferensi serupa.

Kedua pendekatan ini saling melengkapi dan diharapkan dapat memberikan rekomendasi yang lebih personal, fleksibel, dan efektif untuk berbagai tipe pengguna.

# Data Understanding

Dalam proyek ini, digunakan dataset berjudul ["The Movies Dataset"](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset) dari Kaggle, yang berisi metadata film serta interaksi pengguna dalam bentuk rating. Dataset ini dirancang untuk menganalisis preferensi pengguna terhadap film dan membangun sistem rekomendasi yang dapat menyarankan film secara personal. Tujuan dari dataset ini adalah untuk membantu pengguna menemukan film yang relevan dengan minat mereka berdasarkan konten film dan interaksi pengguna lainnya.

##Variabel dalam Dataset
1. **movies_metadata.csv** digunakan sebagai sumber utama untuk pendekatan content-based filtering.
Dengan total jumlah entri: 28.070 data, file ini berisi metadata lengkap dari film. Berikut adalah seluruh fitur dalam file ini:

    * adult: Menunjukkan apakah film mengandung konten dewasa (boolean)

    * belongs_to_collection: Informasi koleksi film (jika merupakan bagian dari serial/waralaba)

    * budget: Anggaran produksi film

    * genres: Daftar genre film (dalam format string JSON)

    * homepage: URL halaman resmi film

    * id: ID unik film

    * imdb_id: ID film di IMDb

    * original_language: Bahasa asli film

    * original_title: Judul asli film

    * overview: Ringkasan cerita (sinopsis)

    * popularity: Skor popularitas berdasarkan TMDB

    * poster_path: Path poster film dari TMDB

    * production_companies: Daftar perusahaan produksi (format JSON)

    * production_countries: Negara produksi (format JSON)

    * release_date: Tanggal rilis film

    * revenue: Pendapatan kotor film

    * runtime: Durasi film (dalam menit)

    * spoken_languages: Bahasa yang digunakan dalam film (format JSON)

    * status: Status rilis film (seperti “Released”, “Post Production”, dll.)

    * tagline: Kalimat slogan/promosi film

    * title: Judul film

    * video: Menunjukkan apakah film adalah video promosi (boolean)

    * vote_average: Rata-rata rating pengguna

    * vote_count: Jumlah pengguna yang memberikan rating

2. **ratings_small.csv** digunakan untuk membangun model collaborative filtering. Dengan total jumlah entri: 10.0004 data, file ini mencatat interaksi pengguna berupa rating terhadap film. Fitur yang tersedia:

    * userId: ID unik pengguna

    * movieId: ID film berdasarkan MovieLens

    * rating: Nilai rating dari pengguna (skala 0.5–5.0)

    * timestamp: Waktu pemberian rating (format Unix timestamp)

3. **keywords.csv** digunakan untuk memperkuat fitur konten film dalam pendekatan content-based filtering.
Dengan total jumlah entri: 46.419 data, file ini menyajikan kata kunci tematik yang berkaitan dengan film. Fitur yang tersedia:

    * id: ID film (mengacu ke movies_metadata.csv)

    * keywords: Daftar kata kunci (format list dalam string JSON)

## Tahapan Awal Ekplorasi Data
Beberapa tahapan eksplorasi dan pemahaman data yang dilakukan:

* Load Dataset: Memuat seluruh dataset yang digunakan (movies_metadata, ratings_small, keywords)

* Melihat Struktur Dataset: Menampilkan ukuran dataset, tipe data, dan 5 baris teratas dari masing-masing file

* Deskripsi Data Numerik: Mendapatkan ringkasan statistik fitur numerik seperti vote_average, popularity, dan runtime

* Cek Missing Values dan Duplikat: Memastikan apakah terdapat data yang hilang atau baris yang duplikat

* Univariate Exploratory Data Analysis: Mengeksplorasi variabel seperti genre, jumlah rating per film, dan distribusi popularitas film

## Visualisasi yang Dilakukan
* Boxplot: Digunakan untuk mendeteksi outlier pada kolom numerik seperti popularity, vote_average, dan runtime

* Bar Plot: Digunakan untuk melihat distribusi kategori seperti genre film, serta daftar film dengan jumlah rating terbanyak

* Histogram: Digunakan untuk melihat distribusi rating pengguna dan durasi film

## Load Dataset

### 1. Memuat keempat file utama yang digunakan dalam proyek, yaitu movies_metadata.csv, ratings_small.csv, dan keywords.csv.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""Insight: Import library yang digunakan"""

movies = pd.read_csv('movies_metadata.csv', low_memory=False)
print("movies_metadata.csv")
display(movies.head())

"""**Insight:** Dataset ini berisi informasi dasar film seperti judul, sinopsis, genre, dan popularitas. Terdapat kolom bertipe string JSON seperti genres dan belongs_to_collection yang perlu diparsing. Beberapa kolom seperti budget dan homepage memiliki nilai kosong atau tidak konsisten."""

ratings = pd.read_csv('ratings_small.csv')
print("ratings_small.csv")
display(ratings.head())

"""**Insight:** Berisi interaksi pengguna berupa rating terhadap film. Setiap baris mewakili satu penilaian. Dataset ini akan digunakan untuk collaborative filtering."""

keywords = pd.read_csv('keywords.csv')
print("keywords.csv")
display(keywords.head())

"""**Insight:** Berisi daftar kata kunci yang menggambarkan konten film. Nilai disimpan dalam format list JSON string dan perlu diparsing sebelum digunakan dalam content-based filtering.

### 2. Memeriksa struktur dataset untuk melihat jumlah data dan memastikan bahwa data sudah dalam tipe data yang benar
"""

movies.info()

"""**Insight:** Melihat info dataset movies_metadata.csv"""

ratings.info()

"""**Insight:** Melihat info dataset ratings_small.csv"""

keywords.info()

"""**Insight:** Melihat info dataset keywords.csv

## Deskripsi Data Numerik

Menampilkan statistik deskriptif untuk semua kolom termasuk yang betipe data object. Penting untuk mengidentifikasi kemungkinan outlier atau nilai yang tidak konsisten.
"""

movies.describe(include="all")

"""**Insight:** Terdapat banyak nilai nol dan kemungkinan outlier pada kolom revenue dan runtime."""

ratings.describe(include="all")

"""**Insight:** Distribusi userId dan movieId cukup luas, mencerminkan banyak variasi interaksi pengguna."""

keywords.describe(include="all")

"""**Insight:** Hanya memiliki kolom id sebagai numerik. Fokus utama adalah isi kolom keywords yang perlu diproses.

## Missing Values dan Duplikat

### 1. Cek Mising Values
"""

movies.isnull().sum()

ratings.isnull().sum()

keywords.isnull().sum()

"""**Insight:**

* 24,898 data null di kolom belongs_to_collection
* 23,530 data null di kolom homepage
* 12,920 data null di kolom tagline
* 277 data null di kolom overview
* 149 data null di kolom poster_path
* 62 data null di kolom runtime
* 31 data null di kolom release_date
* 39 data null di kolom status
* 12 data null di kolom imdb_id
* 3 data null di kolom original_language, revenue, spoken_languages, title, video, vote_average, vote_count
* 2 data null di kolom popularity, production_companies, production_countries

Tidak ditemukan nilai null pada dataset ratings_small.csv maupun keywords.csv.

### 2. Cek duplikat
"""

movies.duplicated().sum()

ratings.duplicated().sum()

keywords.duplicated().sum()

"""**Insight:**
* Terdapat 10 baris duplikat di dataset movies_metadata.csv

* Tidak ditemukan baris duplikat di dataset ratings_small.csv

* Ditemukan 987 baris duplikat di dataset keywords.csv, yang perlu dibersihkan untuk menghindari pengulangan informasi konten pada film

## Univariate Exploratory Data Analysis

### 1. Visualisasi Distribusi Rating Pengguna
 Bertujuan untuk melihat seberapa sering nilai rating tertentu diberikan oleh pengguna. Ini membantu memahami pola penilaian — apakah pengguna cenderung memberi rating tinggi, rendah, atau rata-rata.
"""

plt.figure(figsize=(8, 5))
sns.countplot(data=ratings, x='rating', color='steelblue')
plt.title('Distribusi Rating Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""**Insight:** Mayoritas pengguna memberikan rating antara 3.0 hingga 4.0, dengan rating 4.0 sebagai yang paling banyak. Ini menunjukkan kecenderungan pengguna memberikan penilaian positif terhadap film yang ditonton.

### 2. Visualisasi Distribusi Genre Film

Bertujuan untuk mengetahui genre apa saja yang paling sering muncul dalam dataset movies_metadata.csv. Ini membantu memahami preferensi umum dan potensi fitur dalam content-based filtering.
"""

import ast
from collections import Counter

# Parsing kolom 'genres' dari string JSON menjadi list, karena kolom genres disimpan dalam format JSON string
def extract_genres(genre_str):
    try:
        genres = ast.literal_eval(genre_str)
        return [g['name'] for g in genres]
    except:
        return []

# Terapkan fungsi
movies['genre_list'] = movies['genres'].apply(extract_genres)

# Hitung frekuensi setiap genre
all_genres = sum(movies['genre_list'], [])  # Flatten list
genre_counts = Counter(all_genres)

# Ubah ke DataFrame
genre_df = pd.DataFrame(genre_counts.items(), columns=['Genre', 'Jumlah']).sort_values(by='Jumlah', ascending=False)

# Visualisasi
plt.figure(figsize=(10,6))
sns.barplot(data=genre_df, y='Genre', x='Jumlah', color='steelblue')
plt.title('Distribusi Genre Film')
plt.xlabel('Jumlah')
plt.ylabel('Genre')
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

"""**Insight:** Genre film yang paling dominan dalam dataset adalah Drama, diikuti oleh Comedy dan Thriller.

### 3. Visualisasi Film dengan Jumlah Rating Terbanyak
 Bertujuan untuk mengetahui film mana yang paling sering diberi rating oleh pengguna, sebagai indikator popularitas dan minat.
"""

# Gabungkan ratings dengan movies untuk mendapatkan nama film
movies['id'] = pd.to_numeric(movies['id'], errors='coerce')
ratings['movieId'] = pd.to_numeric(ratings['movieId'], errors='coerce')

# Merge berdasarkan ID film
merged = ratings.merge(movies, left_on='movieId', right_on='id')

# Hitung jumlah rating per judul
rating_counts = merged['title'].value_counts().head(10)

# Visualisasi
plt.figure(figsize=(10,6))
sns.barplot(y=rating_counts.index, x=rating_counts.values, color='steelblue')
plt.title('10 Film dengan Jumlah Rating Terbanyak')
plt.xlabel('Jumlah Rating')
plt.ylabel('Judul Film')
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

"""**Insight :** Film dengan jumlah rating terbanyak dalam dataset adalah Terminator 3: Rise of the Machines, diikuti oleh The Million Dollar Hotel dan Solaris. Ini menunjukkan film-film tersebut memiliki tingkat interaksi tinggi dari pengguna dan dapat dianggap sebagai film populer dalam konteks dataset ini.

### 4. Visualisasi Distribusi Durasi Film
Bertujuan untuk melihat sebaran durasi film, mengidentifikasi film berdurasi pendek atau panjang ekstrem (outlier), dan menentukan apakah perlu membersihkan data berdasarkan runtime.
"""

plt.figure(figsize=(10,5))
sns.histplot(movies['runtime'].dropna(), bins=50, kde=True, color='steelblue')
plt.title('Distribusi Durasi Film')
plt.xlabel('Durasi (menit)')
plt.ylabel('Jumlah Film')
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""**Insight:** Mayoritas film dalam dataset memiliki durasi antara 80 hingga 120 menit. Terdapat lonjakan anomali pada durasi pendek di bawah 20 menit, serta outlier ekstrem hingga di atas 1000 menit, yang kemungkinan besar merupakan data tidak valid atau kesalahan input.

visualisasi dengan boxplot untuk melihat outliers
"""

plt.figure(figsize=(10, 4))
sns.boxplot(x=movies['runtime'], color='steelblue')
plt.title('Boxplot Durasi Film (Runtime)')
plt.xlabel('Durasi (menit)')
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

"""**Insight:** Boxplot menunjukkan bahwa sebagian besar film berdurasi antara 80 hingga 120 menit. Namun, terdapat banyak outlier di kedua sisi: film sangat pendek di bawah 20 menit dan sangat panjang di atas 200 menit hingga lebih dari 1200 menit. Outlier ini kemungkinan berasal dari kesalahan input, film dokumenter pendek, atau konten non-film seperti episode TV atau trailer.

#Data Preparation

Ditahap ini, dilakukan beberapa proses cleaning dan transforming data
* Menghapus Data yang Null: Membersihkan nilai kosong pada kolom penting seperti title, overview, genres, dan runtime untuk mencegah error saat pemodelan dan menjaga kualitas data.

* Membersihkan Kolom genres dan keywords: Mengubah format string JSON pada kolom genres dan keywords menjadi list of strings agar dapat digunakan dalam content-based filtering.

* Menghapus Outlier pada Kolom runtime: Menghapus durasi film yang terlalu pendek (< 30 menit) atau terlalu panjang (> 300 menit) yang tidak realistis.

* Merge Dataset: Menggabungkan movies_metadata.csv dan keywords.csv untuk memperkaya fitur konten. Hapus juga kolom yang tidak digunakan agar data lebih bersih dan fokus.

* Persiapan Content-Based Filtering: Menggabungkan kolom overview, genres, dan keywords menjadi satu kolom teks (combined_features) yang akan digunakan dalam model content-based filtering.

* Persiapan Collaborative Filtering: Menyiapkan data rating (ratings_small.csv) agar bersih dari nilai tidak valid, dan siap digunakan dalam model collaborative seperti SVD.

## Drop Null

Beberapa kolom penting pada dataset movies_metadata.csv mengandung nilai kosong (null) yang dapat mengganggu proses pemodelan. Oleh karena itu, dilakukan penghapusan baris yang memiliki nilai null khusus pada kolom berikut:

* title: Nama film, penting sebagai identitas utama dalam sistem rekomendasi.

* overview: Deskripsi konten film, digunakan dalam content-based filtering.

* genres: Salah satu fitur utama dalam analisis kemiripan film.

* runtime: Digunakan untuk validasi dan analisis durasi film.
"""

# Hapus baris yang null di kolom penting
movies_clean = movies.dropna(subset=['title', 'overview', 'genres', 'runtime'])

# Cek jumlah data setelah dibersihkan
movies_clean.shape

"""**Insight:** Data dihapus karena memiliki nilai kosong pada kolom penting seperti title, overview, genres, dan runtime. Kolom-kolom ini digunakan secara langsung dalam proses pemodelan content-based filtering, sehingga wajib lengkap agar sistem dapat memberikan rekomendasi yang akurat.

Sementara itu, nilai null pada kolom lain seperti homepage, tagline, belongs_to_collection, dan release_date tidak dibersihkan karena kolom-kolom tersebut tidak digunakan sebagai fitur utama dalam sistem rekomendasi. Menghapus baris berdasarkan kolom yang tidak relevan justru berisiko mengurangi jumlah data secara signifikan tanpa memberikan dampak terhadap kualitas model.

## Membersihkan Kolom genres dan keywords
Kolom genres dan keywords pada dataset movies_metadata.csv dan keywords.csv masing-masing tersimpan dalam format string JSON, seperti:

```
"[{'id': 18, 'name': 'Drama'}, {'id': 35, 'name': 'Comedy'}]"
```

Agar dapat digunakan dalam content-based filtering, string tersebut harus diubah menjadi list of strings, seperti:

`['Drama', 'Comedy']`
"""

import ast

# Buat salinan eksplisit dari movies_clean untuk menghindari warning
movies_clean = movies_clean.copy()

# Fungsi parsing string JSON menjadi list of strings
def parse_features(x):
    try:
        data = ast.literal_eval(x)
        return [d['name'] for d in data]
    except:
        return []

# Terapkan parsing pada kolom 'genres' dan 'keywords'
movies_clean['genres'] = movies_clean['genres'].apply(parse_features)
keywords['keywords'] = keywords['keywords'].apply(parse_features)

# Lihat hasil parsing
movies_clean[['title', 'genres']].head()
keywords[['id', 'keywords']].head()

"""**Insight:** Kolom genres pada dataset movies_metadata.csv dan kolom keywords pada dataset keywords.csv awalnya tersimpan dalam format string JSON. Data ini telah berhasil dikonversi menjadi list of strings yang lebih mudah digunakan dalam analisis berbasis konten. Pembersihan ini diperlukan agar model dapat memproses informasi genre dan kata kunci secara akurat.

## Menghapus Outlier pada Kolom runtime

Tujuannya adalah menghindari pengaruh data tidak logis seperti:

* Film dengan durasi terlalu pendek (misalnya < 30 menit)

* Film dengan durasi sangat panjang (> 300 menit)
"""

# Buang outlier dengan durasi < 30 atau > 300 menit
movies_clean = movies_clean[(movies_clean['runtime'] >= 30) & (movies_clean['runtime'] <= 300)]

# Cek jumlah data setelah dibersihkan
movies_clean.shape

"""**Insight:** Film dihapus karena memiliki durasi di luar rentang 30–300 menit. Pembersihan ini bertujuan untuk menghindari pengaruh nilai ekstrem yang dapat merusak akurasi model, dan memastikan fitur runtime mencerminkan film berdurasi wajar.

## Merge Dataset
Menggabungkan informasi dari dua sumber:

* movies_clean: berisi metadata film seperti title, overview, genres, dan runtime

* keywords: berisi daftar kata kunci tematik untuk setiap film

Penggabungan dilakukan berdasarkan kolom id agar setiap film memiliki representasi konten yang lebih kaya.
"""

# memastikan kolom 'id' bertipe numerik agar bisa digabung
movies_clean['id'] = pd.to_numeric(movies_clean['id'], errors='coerce')
keywords['id'] = pd.to_numeric(keywords['id'], errors='coerce')

# Gabungkan dataset berdasarkan 'id'
movies_merged = pd.merge(movies_clean, keywords, on='id', how='left')

# Cek hasil gabungan
movies_merged.shape
movies_merged[['title', 'genres', 'keywords']].head()

"""**Insight:** Dataset movies_merged menyatukan metadata film dan kata kunci, sehingga setiap film kini memiliki fitur konten yang lebih lengkap. Kolom genres dan keywords akan digunakan bersama dengan overview untuk membentuk fitur gabungan dalam model content-based filtering.

## Content-Based Filtering Preparation

Tahap ini bertujuan untuk menyiapkan data yang akan digunakan dalam model Content-Based Filtering, di mana rekomendasi film diberikan berdasarkan kemiripan konten antar film seperti genre, sinopsis, dan kata kunci.

Pendekatan ini tidak memerlukan data interaksi pengguna, tetapi bergantung pada deskripsi dan metadata film. Untuk itu, dilakukan penggabungan berbagai fitur konten menjadi satu representasi teks yang dapat dianalisis secara numerik menggunakan teknik vektorisasi teks.

Pertama,  gabungkan isi kolom genres, keywords, dan overview menjadi satu string agar dapat diproses oleh algoritma teks (TF-IDF + cosine similarity).
"""

# Pastikan keywords sudah list, bukan NaN
movies_merged['keywords'] = movies_merged['keywords'].apply(lambda x: x if isinstance(x, list) else [])

# Gabungkan semua fitur menjadi string
def combine_features(row):
    return ' '.join(row['genres']) + ' ' + ' '.join(row['keywords']) + ' ' + str(row['overview'])

# Buat kolom baru
movies_merged['combined_features'] = movies_merged.apply(combine_features, axis=1)

"""Setelah kolom `combined_features` berhasil dibuat, langkah berikutnya adalah mentransformasikan teks tersebut ke dalam bentuk vektor numerik agar dapat digunakan dalam perhitungan kemiripan antar film. Teknik yang digunakan untuk representasi teks ini adalah **TF-IDF (Term Frequency-Inverse Document Frequency)**.

TF-IDF menghitung bobot pentingnya kata dalam sebuah dokumen (dalam hal ini film), relatif terhadap semua film lainnya di dataset. Semakin unik sebuah kata dalam sebuah film, semakin tinggi bobotnya. Hal ini memungkinkan model mengenali kata kunci khas dari setiap film.

Proses ini dilakukan menggunakan `TfidfVectorizer` dari library `sklearn`, dengan batas maksimum 3000 fitur dan penghapusan stopwords untuk menjaga efisiensi memori dan fokus pada kata-kata penting.

"""

# Buat dataframe yang hanya berisi data yang akan digunakan dalam pemodelan
content_based_df = pd.DataFrame({
    'id': movies_merged['id'],
    'title': movies_merged['title'],
    'combined_features': movies_merged['combined_features']
})

# Lihat isi awal
content_based_df.head()

"""**Insight:**
Dataset content_based_df berhasil dibentuk sebagai struktur minimal dan efisien yang hanya berisi informasi yang dibutuhkan untuk model content-based filtering. Kolom combined_features yang telah dibuat menggabungkan genre, kata kunci, dan ringkasan cerita, sehingga bisa digunakan untuk mengukur kemiripan antar film secara tekstual. Struktur ini memudahkan saat proses vektorisasi dan perhitungan cosine similarity nantinya.


"""

print(len(content_based_df))

"""**Insight:** Jumlah data pada `content_based_df` adalah **26.946**, yang artinya:

* Proses pembersihan dan penggabungan fitur berhasil dilakukan.
* Meskipun sebagian data difilter atau dibersihkan, jumlah baris tetap cukup besar dan representatif untuk proses rekomendasi.
* Semua fitur utama telah siap digunakan dalam pendekatan content-based filtering.

*Disclaimer:
Untuk menghindari kendala teknis terkait keterbatasan memori (RAM) pada perangkat lokal, proses content-based filtering dalam proyek ini dijalankan menggunakan subset data berjumlah 5.000 film yang diambil secara acak dari keseluruhan dataset. Pendekatan ini tetap mewakili struktur data dan logika sistem rekomendasi secara menyeluruh, serta tidak memengaruhi kualitas implementasi. Jika dijalankan pada perangkat dengan spesifikasi lebih tinggi, sistem dapat diperluas untuk mencakup seluruh dataset*

Sampling data, dilakukan pemilihan secara acak terhadap 5.000 data film dari content_based_df dengan random_state=42 agar hasil sampling dapat direproduksi. Hal ini dilakukan agar sistem tetap dapat berjalan optimal di perangkat dengan memori terbatas, tanpa memproses seluruh 42.000+ data film.
"""

# Ambil 5000 film acak dari content_based_df
sample_df = content_based_df.sample(5000, random_state=42).reset_index(drop=True)

"""**Insight:** Diambil 5000 film secara acak dari dataset untuk menghindari crash akibat keterbatasan RAM.

TF-IDF Vectorization, Langkah ini mengubah teks gabungan (genre + keywords + sinopsis) menjadi representasi numerik menggunakan TF-IDF. Jumlah maksimal kata penting dibatasi 3.000 untuk menjaga efisiensi memori. TF-IDF menekankan kata-kata unik dalam tiap film dibandingkan seluruh koleksi film.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Batasi hanya 3000 fitur kata untuk efisiensi memori
tfidf = TfidfVectorizer(stop_words='english', max_features=3000)
tfidf_matrix = tfidf.fit_transform(sample_df['combined_features'])

print(tfidf_matrix.shape)  # Contoh: (5000, 3000)

"""**Insight:** Diterapkan TF-IDF di tahap Data Preparation kemudian digunakan untuk menghitung kesamaan antar film menggunakan cosine similarity dengan maksimal 3000 fitur untuk merepresentasikan teks secara efisien tanpa membebani memori.n T.

## Collaborative Filtering Preparation

Tahap ini bertujuan untuk menyiapkan data yang akan digunakan dalam model Collaborative Filtering, di mana rekomendasi film diberikan berdasarkan kemiripan pola rating antar pengguna.

Dalam model Collaborative Filtering berbasis deep learning, sistem tidak dapat langsung memahami data dalam format string seperti userId dan movieId. Oleh karena itu, dilakukan proses encoding untuk mengubah ID pengguna dan ID film menjadi representasi numerik menggunakan teknik enumerate(), yang menghasilkan mapping dari ID asli ke indeks numerik yang dapat diproses oleh model.

Selain itu, dilakukan juga normalisasi nilai rating agar semua nilai berada dalam skala 0–1. Normalisasi ini bertujuan untuk membantu proses pelatihan model menjadi lebih stabil dan efisien.

Ambil dan salin data yang dibutuhkan, data rating dari pengguna digabungkan dengan judul film agar lebih informatif, lalu disederhanakan hanya ke kolom userId, movieId, title, dan rating untuk memudahkan pemrosesan lebih lanjut.
"""

collaborative_df = ratings.copy()
collaborative_df = collaborative_df.merge(movies_clean[['id', 'title']], left_on='movieId', right_on='id', how='left')
collaborative_df = collaborative_df[['userId', 'movieId', 'title', 'rating']].copy()
collaborative_df.head()

"""Encoding dilakukan pada userId dan movieId karena model deep learning tidak dapat memahami data dalam bentuk string. Oleh karena itu, ID pengguna dan ID film diubah menjadi representasi numerik menggunakan teknik enumerate(), yang menghasilkan dictionary sebagai mapping dari ID asli ke bentuk indeks numerik yang dapat diproses oleh model."""

# Buat mapping user dan movie ke index numerik
user_ids = collaborative_df['userId'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

movie_ids = collaborative_df['movieId'].unique().tolist()
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

""" Mapping ke kolom baru, kolom baru user dan movie disiapkan untuk digunakan oleh model. Nilai rating dikonversi ke float32 agar kompatibel dengan model TensorFlow."""

# Mapping userId dan movieId ke encoding
collaborative_df['user'] = collaborative_df['userId'].map(user_to_user_encoded)
collaborative_df['movie'] = collaborative_df['movieId'].map(movie_to_movie_encoded)

# Pastikan rating berbentuk float
collaborative_df['rating'] = collaborative_df['rating'].values.astype(np.float32)

"""Menampilkan jumlah total pengguna dan film unik yang akan digunakan oleh model, serta memeriksa skala rating awal (antara 0.5–5.0). Informasi ini penting sebelum dilakukan normalisasi ke skala 0–1."""

# Jumlah user, movie, dan skala rating
num_users = len(user_to_user_encoded)
num_movies = len(movie_to_movie_encoded)

min_rating = collaborative_df['rating'].min()
max_rating = collaborative_df['rating'].max()

print(f'Number of Users: {num_users}, Number of Movies: {num_movies}, Min Rating: {min_rating}, Max Rating: {max_rating}')

"""Nilai rating dinormalisasi agar berada dalam rentang 0–1 menggunakan min-max normalization, sehingga model tidak bias terhadap skala asli rating. Setelah itu, data dibagi menjadi dua subset: 80% untuk data pelatihan dan 20% untuk data validasi. Pembagian ini dilakukan untuk mengukur performa model pada data yang belum pernah dilihat saat pelatihan."""

# Buat X dan y
x = collaborative_df[['user', 'movie']].values
y = collaborative_df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Split train dan validation
train_indices = int(0.8 * collaborative_df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(f'Dataset: {x.shape}')
print(f'Data train: {x_train.shape}')
print(f'Data validasi: {x_val.shape}')

"""**Insight:** Dataset collaborative filtering berhasil disiapkan dengan mengubah userId dan movieId menjadi representasi numerik. Data kemudian dibagi menjadi 80% untuk pelatihan dan 20% untuk validasi, sehingga siap digunakan dalam model rekomendasi berbasis interaksi pengguna.

# Modeling and Results

## 1. Content-Based Filtering
Content-Based Filtering adalah pendekatan sistem rekomendasi yang menganalisis karakteristik konten dari item, dalam hal ini film, seperti genre, sinopsis, dan kata kunci. Sistem ini merekomendasikan film lain yang memiliki konten serupa dengan film yang pernah disukai pengguna, tanpa bergantung pada interaksi pengguna lain. Karena itu, pendekatan ini sangat cocok untuk mengatasi masalah cold-start pada pengguna baru.

Pendekatan ini tidak membutuhkan data dari pengguna lain, sehingga sangat cocok digunakan untuk pengguna baru yang belum memiliki riwayat interaksi atau rating.

**Cara Kerja Singkat:**

Sistem ini mengubah informasi konten film menjadi representasi numerik dan menghitung kemiripan antar film menggunakan metode pengukuran jarak (misalnya cosine similarity). Film yang paling mirip dengan film yang disukai pengguna akan direkomendasikan.

**Kelebihan:**

* Dapat menangani cold-start pada pengguna baru karena tidak memerlukan data interaksi pengguna lain.

* Rekomendasi yang diberikan lebih personal dan sesuai dengan konten yang relevan.

* Tidak bergantung pada popularitas film, sehingga bisa memberikan saran unik.

**Kelemahan:**

* Kurang eksploratif karena hanya menyarankan film yang mirip secara konten.

* Bergantung pada kelengkapan dan kualitas metadata (sinopsis, genre, keywords, dll).

* Sulit menangani kasus di mana pengguna ingin rekomendasi dari genre yang benar-benar berbeda dari preferensinya sebelumnya.

Menghitung Kemiripan Antar Film (Cosine Similarity) <br>

Setelah fitur konten diubah menjadi representasi vektor, tahap utama dalam Content-Based Filtering adalah menghitung kemiripan antar film. Untuk itu, digunakan metode cosine similarity, yang mengukur kedekatan arah antara dua vektor teks.
"""

from sklearn.metrics.pairwise import linear_kernel

# Lebih ringan dibanding cosine_similarity
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

"""**Insight:** Linear kernel digunakan sebagai alternatif cosine similarity untuk efisiensi pemrosesan yang lebih hemat RAM.

Membentuk Matriks Kemiripan Antar Film <br>
Matriks cosine similarity dikonversi menjadi DataFrame agar memudahkan pencarian dan pemetaan kemiripan antar judul film. Baris dan kolom berisi nama film, dan setiap sel berisi nilai kemiripan (dalam rentang 0–1).
"""

import pandas as pd

cosine_sim_df = pd.DataFrame(cosine_sim, index=sample_df['title'], columns=sample_df['title'])
cosine_sim_df.sample(3, axis=1).sample(5, axis=0)  # Contoh cuplikan

"""**Insight:** Dibuat matriks kemiripan antar judul film yang bisa digunakan untuk menampilkan skor kesamaan film.

Fungsi Rekomendasi Film <br>
Fungsi ini menerima input berupa judul film dan mengembalikan Top-10 film yang paling mirip berdasarkan skor cosine similarity. Fungsi ini menghindari merekomendasikan film yang sama dengan input dan dapat diintegrasikan dengan antarmuka pengguna secara langsung.
"""

def movie_recommendations(title, similarity_data=cosine_sim_df, items=sample_df[['title']], k=10):
    if title not in similarity_data.columns:
        return f"Judul '{title}' tidak ditemukan di data."
    index = similarity_data.loc[:, title].to_numpy().argpartition(range(-1, -k-1, -1))
    closest = similarity_data.columns[index[-k-1:]][::-1]
    closest = closest.drop(title, errors='ignore')
    return pd.DataFrame(closest).merge(items).drop_duplicates().head(k)

"""**Insight:** Fungsi movie_recommendations berhasil dibuat untuk mengambil top-10 film yang paling mirip berdasarkan skor kemiripan.

Contoh Penggunaan Fungsi Rekomendasi, contoh penggunaan fungsi movie_recommendations dengan judul "Toys in the Attic" untuk menampilkan 10 film lain yang paling mirip secara konten. Hasil rekomendasi biasanya memiliki tema fantasi, anak-anak, atau petualangan — sesuai dengan konten film acuan.
"""

sample_df[sample_df['title'].str.contains("toy", case=False, na=False)]

"""**Insight:** Mencari judul dengan keyword toy"""

recs = movie_recommendations("Toys in the Attic")
recs

"""**Insight:** Ketika diberikan input “Toys in the Attic”, sistem berhasil memberikan rekomendasi seperti “The Lorax”, “Brother Bear”, dan “The Legend of Sarila” yang memiliki nuansa serupa: animasi, petualangan, dan ramah anak. Ini menunjukkan bahwa sistem dapat mengenali pola konten dengan baik dan memberikan saran yang relevan berdasarkan kemiripan semantik.

## 2. Collaborative Filtering

Collaborative Filtering adalah pendekatan yang merekomendasikan item (dalam hal ini film) berdasarkan interaksi pengguna seperti rating. Model akan belajar dari pola kesukaan banyak pengguna, lalu menyarankan film kepada pengguna lain yang memiliki preferensi serupa.

**Kelebihan:**
* Tidak membutuhkan informasi konten film

* Bisa mengungkap pola hubungan tersembunyi antar film

* Cocok untuk dataset dengan banyak data interaksi pengguna

**Kelemahan:**
* Tidak cocok untuk pengguna/item baru (cold-start)

* Membutuhkan data interaksi yang cukup banyak untuk bekerja optimal

**Arsitektur Model (RecommenderNet)**
Menggunakan model deep learning berbasis embedding, dengan komponen:

* user_embedding dan movie_embedding: mengubah ID user dan movie jadi vektor

* user_bias dan movie_bias: membantu model lebih fleksibel

* dot_user_movie: mengukur kedekatan user dan movie

* sigmoid: mengubah skor jadi nilai antara 0–1

Mendefinisikan model rekomendasi berbasis TensorFlow bernama RecommenderNet. Model ini menggunakan teknik embedding untuk memetakan ID pengguna dan ID film ke dalam representasi vektor berdimensi rendah. Setiap pengguna dan film juga diberikan bias masing-masing. Produk dot antara embedding pengguna dan film dihitung, lalu ditambahkan dengan bias sebelum diterapkan fungsi aktivasi sigmoid. Dropout ditambahkan untuk regularisasi dan mencegah overfitting.
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_movies, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(num_users, embedding_size, embeddings_initializer='uniform', embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.movie_embedding = layers.Embedding(num_movies, embedding_size, embeddings_initializer='uniform', embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.user_bias = layers.Embedding(num_users, 1)
        self.movie_bias = layers.Embedding(num_movies, 1)
        self.dropout = layers.Dropout(0.3)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        user_bias = self.user_bias(inputs[:, 0])
        movie_bias = self.movie_bias(inputs[:, 1])
        dot_user_movie = tf.reduce_sum(user_vector * movie_vector, axis=1, keepdims=True)
        x = dot_user_movie + user_bias + movie_bias
        return tf.nn.sigmoid(self.dropout(x))

"""Kompilasi dan Training Model

Model dikompilasi menggunakan fungsi loss Mean Squared Error (MSE) yang umum dipakai untuk regresi seperti prediksi rating. Optimizer RMSprop digunakan dengan learning rate 0.001. Selain itu, metrik evaluasi yang dipantau adalah Root Mean Squared Error (RMSE). Model kemudian dilatih selama 10 epoch dengan data training (x_train, y_train) dan divalidasi menggunakan data validasi (x_val, y_val). Pelatihan ini memungkinkan model belajar hubungan antara pengguna dan film berdasarkan pola interaksi yang ada.
"""

model = RecommenderNet(num_users, num_movies, embedding_size=10)
model.compile(
    loss=tf.keras.losses.MeanSquaredError(),
    optimizer=keras.optimizers.RMSprop(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x_train,
    y_train,
    batch_size=512,
    epochs=10,
    validation_data=(x_val, y_val)
)

"""**Insight:**
* Model berhasil menurunkan error prediksi (RMSE) dari epoch 1 hingga 10.

* RMSE akhir data validasi sebesar 0.2678 menandakan prediksi model cukup akurat untuk skala 0–1.

* Model menunjukkan stabilitas yang baik tanpa lonjakan error → menandakan bahwa model tidak overfitting.

* Model siap digunakan untuk memberikan rekomendasi film kepada pengguna berdasarkan pola rating mereka.

Setelah model collaborative filtering selesai dilatih, langkah selanjutnya adalah menghasilkan rekomendasi film berdasarkan pola kesukaan pengguna. Proses dimulai dengan memilih satu userId secara acak dari dataset rating. Kemudian, diambil daftar film yang sudah pernah dirating oleh pengguna tersebut. Dari situ, disusun daftar film yang belum dirating—yaitu film yang belum ditonton oleh pengguna dan tersedia dalam data (termasuk dalam pemetaan movie_to_movie_encoded). Film-film inilah yang akan digunakan untuk prediksi dan rekomendasi.

Pilih Satu User dan Siapkan Data Film yang Belum Dirating
"""

# Ambil satu user ID secara acak
user_id = ratings['userId'].sample(1).iloc[0]

# Ambil daftar film yang sudah dirating user
movies_rated_by_user = ratings[ratings['userId'] == user_id]

# Ambil daftar film yang belum dirating user
valid_movie_ids = set(movie_to_movie_encoded.keys())
movies_not_rated = [
    movie_id for movie_id in movies['id'].unique()
    if (movie_id not in movies_rated_by_user['movieId'].values) and (movie_id in valid_movie_ids)
]

"""Encode User dan Movie, Buat Array Prediksi

Film yang belum ditonton oleh user diencode menggunakan movie_to_movie_encoded, lalu digabung dengan ID user yang telah diencode untuk membentuk pasangan [user, movie]. Hasilnya adalah array input yang siap digunakan oleh model untuk memprediksi rating terhadap setiap film.
"""

movies_not_rated_encoded = [[movie_to_movie_encoded[x]] for x in movies_not_rated]
user_encoded = user_to_user_encoded.get(user_id)

# Siapkan input untuk model prediksi
user_movie_array = np.hstack(([[user_encoded]] * len(movies_not_rated_encoded), movies_not_rated_encoded)).astype(np.float32)

"""Buat Fungsi Rekomendasi Film

Fungsi ini memprediksi rating terhadap film yang belum ditonton user, lalu mengambil top-k film dengan skor tertinggi sebagai rekomendasi. ID film hasil prediksi dikonversi kembali ke ID asli, kemudian dicocokkan dengan metadata dari dataset movies untuk menampilkan judulnya. Rekomendasi ditampilkan dalam format daftar terurut, dan juga dikembalikan sebagai DataFrame.
"""

def get_movie_recommendations(user_id, model, user_movie_array, movies_not_rated_encoded, movie_encoded_to_movie, movies, k=10):
    # Prediksi rating untuk setiap film
    predicted_ratings = model.predict(user_movie_array).flatten()

    # Ambil top-k indeks berdasarkan skor tertinggi
    top_indices = predicted_ratings.argsort()[-k:][::-1]

    # Ambil movieId asli dari hasil encoding
    recommended_movie_ids = [movie_encoded_to_movie[movies_not_rated_encoded[x][0]] for x in top_indices]

    # Ambil metadata film dari movies.csv
    recommended_movies = movies[movies['id'].isin(recommended_movie_ids)][['title']].drop_duplicates().head(k)

    print(f"\nTop {k} Movie Recommendations for User {user_id}:\n" + "="*50)
    for i, title in enumerate(recommended_movies['title'], 1):
        print(f"{i}. {title}")

    return recommended_movies

recommendations = get_movie_recommendations(
    user_id=user_id,
    model=model,
    user_movie_array=user_movie_array,
    movies_not_rated_encoded=movies_not_rated_encoded,
    movie_encoded_to_movie=movie_encoded_to_movie,
    movies=movies,
    k=10
)

"""**Insight:**

Rekomendasi yang dihasilkan untuk user ID 95 menunjukkan variasi film dari berbagai genre, mulai dari drama, crime, hingga action. Hal ini menunjukkan bahwa model collaborative filtering mampu menangkap preferensi tersembunyi pengguna berdasarkan pola interaksi pengguna lain yang memiliki kesamaan selera.

Beberapa film seperti “Once Were Warriors” dan “Three Colors: Red” menunjukkan bahwa pengguna ini cenderung menyukai film dengan tema emosional dan mendalam, sementara film seperti “Men in Black II” dan “48 Hrs.” mengindikasikan adanya preferensi terhadap film aksi dan komedi.

Secara keseluruhan, model berhasil memberikan rekomendasi yang beragam namun masih relevan, yang merupakan kekuatan utama dari pendekatan collaborative filtering.

# Evaluation

## Content-Based Filtering

Evaluasi dilakukan dengan menggunakan Precision, yaitu ukuran yang menghitung tingkat relevansi dari hasil rekomendasi yang dihasilkan oleh sistem.
Metrik ini menilai berapa banyak film yang direkomendasikan benar-benar cocok atau sesuai dengan preferensi pengguna.

**Mengapa Memilih Precision?**

Precision sangat berguna ketika kita ingin tahu seberapa akurat sistem dalam memberikan rekomendasi.
Fokusnya adalah pada kualitas dari hasil rekomendasi—bukan kuantitas. Jadi, meskipun sistem tidak memberikan banyak rekomendasi, asalkan rekomendasinya benar-benar tepat, nilainya tetap tinggi.
Berbeda dengan recall yang menghitung semua kemungkinan relevan (termasuk yang tidak ditampilkan), precision hanya memperhatikan yang ditampilkan.

**Rumus**
$$
\text{Precision} = \frac{\text{Jumlah item relevan yang direkomendasikan}}{\text{Jumlah total rekomendasi yang diberikan}}
$$

**Penjelasan:**
* Jumlah item relevan = jumlah film dalam daftar rekomendasi yang punya kemiripan (genre/kata kunci/overview) dengan film acuan.

* Jumlah total rekomendasi = jumlah film yang ditampilkan oleh sistem kepada pengguna.

* Precision = rasio dari rekomendasi yang relevan terhadap seluruh rekomendasi yang diberikan.

Membuat Fungsi Precision
"""

def precision_from_recommendations(query_title, recommendations):
    # Tokenisasi kata dari judul input
    query_tokens = set(query_title.lower().split())

    # Hitung berapa rekomendasi yang judulnya punya token serupa
    relevant_count = sum(
        len(query_tokens.intersection(set(str(title).lower().split()))) > 0
        for title in recommendations['title']
    )

    return relevant_count / len(recommendations)

"""Menghitung Nilai Precision"""

recs = movie_recommendations("Toys in the Attic")
precision = precision_from_recommendations("Toys in the Attic", recs)
print(f"Precision: {precision:.2f}")

"""**Insight:** Nilai precision sebesar 0.30 menunjukkan bahwa 3 dari 10 film yang direkomendasikan memiliki kesamaan kata dengan judul film acuan, yaitu "Toys in the Attic".

Meskipun nilai precision tidak tinggi, hal ini justru mencerminkan kekuatan pendekatan content-based filtering berbasis kemiripan konten (bukan sekadar kemiripan kata). Sistem dirancang untuk menemukan film dengan vibes, suasana, atau tema yang sejenis, bukan hanya film dengan nama mirip.

Oleh karena itu, meskipun judul-judul rekomendasi tampak berbeda secara literal, mereka tetap relevan secara tematik dan atmosfer, sesuai dengan tujuan utama proyek, yaitu membantu pengguna menemukan film yang memiliki nuansa serupa dengan preferensi mereka.

Sistem berhasil memenuhi tujuannya dengan memberikan rekomendasi berdasarkan kesamaan konten dan nuansa, bukan hanya kesamaan kata dalam judul.

## Collaborative Filtering

Root Mean Squared Error (RMSE) digunakan untuk mengukur seberapa besar rata-rata kesalahan prediksi yang dihasilkan oleh model. Metrik ini dihitung dari akar kuadrat selisih kuadrat antara nilai prediksi dan nilai aktual. Karena hasil RMSE berada dalam satuan yang sama dengan target (rating), maka metrik ini lebih mudah diinterpretasikan dan memberikan gambaran yang konkret terhadap kinerja model.

**Mengapa Memilih RMSE?**

RMSE merupakan metrik yang sensitif terhadap kesalahan besar (outlier) karena menggunakan kuadrat dari selisih prediksi dan data asli. Oleh karena itu, metrik ini sangat cocok digunakan jika kita ingin memastikan bahwa prediksi model tidak hanya akurat secara rata-rata, tetapi juga tidak memiliki kesalahan besar yang tersembunyi. Dalam konteks sistem rekomendasi, RMSE memberikan informasi penting tentang seberapa dekat skor prediksi sistem terhadap penilaian aktual pengguna.

**Rumus:**

$$
RMSE = \sqrt{ \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2 }
$$

**Keterangan:**
- $\hat{y}_i$ = nilai prediksi oleh model  
- $y_i$ = nilai aktual dari data  
- $n$ = jumlah total data

**Visualisasi:**

Learning curve digunakan untuk melihat perkembangan performa model pada data pelatihan dan validasi selama proses training. Grafik ini dapat membantu mengidentifikasi apakah model mengalami underfitting, overfitting, atau sudah optimal.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('Learning Curve - RMSE')
plt.ylabel('Root Mean Squared Error')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

"""**Insight:**
* Grafik menunjukkan penurunan konsisten pada nilai RMSE baik untuk data latih maupun validasi, menandakan bahwa model terus belajar dan meningkatkan akurasi prediksinya seiring bertambahnya epoch.

* Garis tren pada data validasi tetap lebih tinggi dari data latih, tetapi keduanya menurun paralel, menunjukkan bahwa model tidak overfitting secara ekstrem.

* Perbedaan nilai RMSE yang tidak terlalu jauh antara data pelatihan (0.218) dan validasi (0.268) mengindikasikan bahwa model memiliki kemampuan generalisasi yang cukup baik.

# Conclusion

**Kesimpulan:**
* Model Content-Based Filtering berhasil memberikan rekomendasi film yang relevan berdasarkan kemiripan konten (judul, genre, dan sinopsis) dengan pendekatan TF-IDF dan cosine similarity. Dari hasil evaluasi dengan precision sebesar 0.30, sistem mampu menghasilkan beberapa rekomendasi yang memiliki kesamaan konteks atau tema dengan film yang dicari, seperti "Toys in the Attic".

* Model Collaborative Filtering memanfaatkan interaksi pengguna berupa rating untuk menemukan pola preferensi dan memberikan saran film lain yang disukai pengguna serupa. Model ini mampu menghasilkan rekomendasi yang bervariasi dan personal. Berdasarkan evaluasi menggunakan Root Mean Squared Error (RMSE), model memperoleh nilai:

  * RMSE (Training): 0.218

  * RMSE (Validation): 0.268

Ini menunjukkan performa prediksi yang cukup baik, meskipun terdapat sedikit gap antara training dan validation.

**Saran Pengembangan:**
* Precision pada Content-Based Filtering dapat ditingkatkan dengan menambahkan fitur lain seperti aktor, sutradara, atau production company ke dalam representasi konten.

* Untuk Collaborative Filtering, dapat dikembangkan dengan pendekatan matrix factorization atau hybrid model agar bisa menangani cold-start problem dan memperkuat akurasi.

* Perlu penyesuaian threshold similarity atau pengayaan metadata film untuk menghasilkan rekomendasi yang lebih personal dan kontekstual.
"""